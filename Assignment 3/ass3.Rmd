---
title: "Regression 2"
author: "Saha Debanshee Gopal"
date: "11/07/2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#The Need for Big Data?
Statistical softwares like Base R and SAS reads data by default into your memory(RAM).It's easy to exhaust RAM by storing unnecessary information. Our OS and architecture can access only 4GB of data of memory on 32 bit system. And over dumping of data can slow down your systems.

Hence we need robust systems to handle big data. Our systems should be able to tackle data in terms of Volume, Variety and Velocity of the Data.

The Volume of data is continuously growing. We have huge feeds of data being generated everyday e.g. from social networking sites like your twitter ,Facebook,etc.So Data storage and Data Processing is the need of the hour. Also your conventional databases like Teradata, SQL server, etc. cannot handle huge Volumes of data (Terabytes of data).Velocity signifies the speed with which we can handle data. Another typical problem which data analysts are facing today is to load and analyse unstructured forms of data like text, graphics, etc.

So different kinds of Big Data systems are being designed to handle all these glitches present in our traditional systems.

#Data generation : 

##450 MB File
n1 <- 10000000
x1 <- 1:n1
x2 <-runif(n1,5,95)
x3 <- rbinom(n1,1,.4)
x4 <- rnorm(n1, mean=-30, sd=200)
x5 <- runif(n1,-5000,5000)
b0 <- 17; b1 <- -0.466; b2 <- 0.037; b3 <- -5.2; b4 <- 2; b5 <- 0.00876
sigma <- 1.4
epsilon <- rnorm(x1,0,sigma)
y <- b0 + b1*x1  + b2*x2  + b3*x3 + b4*x4 + b5*x5 + x1*x2 + epsilon
data14a<-cbind(y,x1,x2,x3,x4,x5)


##4.5 GB File
n1 <- 50000000
x1 <- 1:n1
x2 <-runif(n1,5,95)
x3 <- rbinom(n1,1,.4)
x4 <- rnorm(n1, mean=-30, sd=200)
x5 <- runif(n1,-5000,5000)
b0 <- 17; b1 <- -0.466; b2 <- 0.037; b3 <- -5.2; b4 <- 2; b5 <- 0.00876
sigma <- 1.4
epsilon <- rnorm(x1,0,sigma)
y <- b0 + b1*x1  + b2*x2  + b3*x3 + b4*x4 + b5*x5 + x1*x2 + epsilon
data14a<-cbind(y,x1,x2,x3,x4,x5)

```{r}
d1 <- read.csv("C:/Users/hp/Desktop/data_small.txt",sep = "")
```
#LM :
```{r}

lm1 <- lm(y~x1+x2+x3+x4+x5,data=d1)
summary(lm1)
```
***Conventional lm() and glm() functions often fail when dealing with large datasets because of the limited availability of memory***
lm will work for 885 mb file but won't work for 4.5 Gb file.



#BIGLM : 
```{r}
library(biglm)

mod1 <- biglm(y~x1+x2+x3+x4+x5,data = d1)
summary(mod1)

mod2 <- bigglm(y~x1+x2+x3+x4+x5,data = d1)
summary(mod2)

family(mod2)
deviance(mod2)
AIC(mod2,k=2)
sdd<-summary(mod2)$sigma


```
***biglm() and bigglm(), from the package biglm by Thomas Lumley. They fit LMs and GLMs using an amount of memory which is only O(p^2),where p is the number of covariates.***

```{r}
library(hier.part)
env <- d1[,2:6]
all.regs(d1$y, env, fam = "gaussian", gof = "Rsqu",
         print.vars = TRUE)

```


#BIG MEMORY/BIG ANALYTICS

This package consists of:

big.matrix
is an R object that simply points to a data structure in C++. Local to a single R process and is limited by available RAM.

shared.big.matrix is similar, but can be shared among multiple R processes (similar to parallelism on data)

filebacked.big.matrix does not point to a data structure; instead it points to a file on disk containing the matrix, and the file can be shared across a cluster
The major advantages of using this package is:

Can store a matrix in memory, restart R, and gain access to the matrix without reloading data. Great for big data.
Can share the matrix among multiple R instances or sessions.
Access is fast because RAM is fast. C++ also helps.
One ***disadvantage*** could be that that matrices contain only one type of data.
```{r}
library(bigmemory)
library(biglm)

library(biganalytics)
zz <- biglm.big.matrix(y ~ x1 +x2+x3+x4+x5,data = d1)
summary(zz)


yy <- bigglm.big.matrix(y ~ x1 +x2+x3+x4+x5,data = d1)
sdd1<-summary(yy)$sigma
sd2<-sdd1*sqrt(10000000)
sd2

```

#FF/BIG ANALYTICS

In bigmemory R keeps a pointer to a C++ matrix. The matrix is stored in RAM or on disk. In ff,R keeps metadata about the object, and the object is stored in a at binary file.

Advantages:

Allows R to work with multiple large Datasets. It also helps us to clean the system and not make a mess with tons of files. For modelling purposes, ffbase has bigglm.ffdf to allow to build generalized linear models easily on large data and can connect to the stream package for clustering & classification.

So if you had to make a choice between using bigmemory or ff, you can choose either as they both contribute similar performance.


```{r}
library(ff)
library(ffbase)

library(biglm)
library(biganalytics)

ddh <- bigglm.ffdf(y ~ x1 +x2+x3+x4+x5,data = d1)
sdd2<-summary(ddh)$sigma
sd3<-sdd2*sqrt(10000000)
sd3


```

#RNetCDF

Is a package for reading and writing NetCDF Datasets.
NetCDF is a widely used file format in atmospheric and oceanic research,especially for weather and climate model output,which allows storage of different types of array based data, along with a short data description.


#DBI/RSQLite/sqldf/RPostgreSQL/RODBC



***DBI***: A database interface (DBI) definition for communication between R and relational database management systems. All classes in this package are virtual and need to be extended by the various R/DBMS implementations

***RSQLite***: This package embeds the SQLite database engine in R and provides an interface compliant with the DBI package. The source for the SQLite engine is included.

***sqldf***: This one is an outlier: Manipulate R data frames using SQL
We cannot index a csv file or a data.frame. If you have to 
> repeatedly select subsets of your large data set, creating an index on the 
> relevant column in the sqlite table is an absolute life saver.

This is one reason the data.table package was created.It is very similar to a data.frame, with the addition of things like keys.If you consider an index in sqlite a life saver then data.table might be up your street.

Its really simple to do this in R.

It is extremely easy to use, and can be of great value to developers who need a database available but want to avoid the overhead often associated with installing and configuring an external database
```{r}
set.seed(123);
n <- 5000;
p <- 5;
x <- matrix(rnorm(n * p), n, p);
x <- cbind(1, x);
bet <- c(2, rep(1, p));
y <- c(x %*% bet) + rnorm(n);

t1 <- Sys.time();
beta.hat <- solve(t(x) %*% x, t(x) %*% y);
t2 <- Sys.time();

gc();
dat <- as.data.frame(x);

rm(x);
gc();
dat$y <- y;
rm(y);
gc();
colnames(dat) <- c(paste("x", 0:p, sep = ""), "y");
head(dat)
gc();

t3 <- Sys.time();
# Will also load the DBI package
library(RSQLite);
# Using the SQLite database driver
m <- dbDriver("SQLite");
# The name of the database file
dbfile <- "regression.db";
# Create a connection to the database
con <- dbConnect(m, dbname = dbfile);
# Write the data in R into database
if(dbExistsTable(con, "regdata")) dbRemoveTable(con, "regdata");
dbWriteTable(con, "regdata", dat, row.names = FALSE);
# Close the connection
dbDisconnect(con);
# Garbage collection
t4 <- Sys.time();
rm(dat);
gc();

t5 <- Sys.time();
m <- dbDriver("SQLite");
dbfile <- "C:/Users/hp/Desktop/regression.db";
con <- dbConnect(m, dbname = dbfile);
# Get variable names
vars <- dbListFields(con, "regdata");
xnames <- vars[-length(vars)];
yname <- vars[length(vars)];
# Generate SQL statements to compute X'X
mult <- outer(xnames, xnames, paste, sep = "*");
lower.index <- lower.tri(mult, TRUE);
mult.lower <- mult[lower.index];
sql <- paste("sum(", mult.lower, ")", sep = "", collapse = ",");
sql <- sprintf("select %s from regdata", sql);
txx.lower <- unlist(dbGetQuery(con, sql), use.names = FALSE);



txx <- matrix(0, p + 1, p + 1);
txx[lower.index] <- txx.lower;
txx <- t(txx);
txx[lower.index] <- txx.lower;
# Generate SQL statements to compute X'Y
sql <- paste(xnames, yname, sep = "*");
sql <- paste("sum(", sql, ")", sep = "", collapse = ",");
sql <- sprintf("select %s from regdata", sql);
txy <- unlist(dbGetQuery(con, sql), use.names = FALSE);

txy <- matrix(txy, p + 1);

# Compute beta hat in R
beta.hat.DB <- solve(txx, txy);
t6 <- Sys.time();
dbDisconnect(con);

beta.hat

beta.hat.DB

max(abs(beta.hat - beta.hat.DB));
t2 - t1;
t4 - t3;
t6 - t5;

?t()

```


#SNOW/SNOWFALL

***Snow*** (Simple network of Workstations) provides an interface to several parallelization packages like MPI, PVM (Parallel Virtual Machines, etc.).All of these systems allow intrasystem communication for working with multiple CPUs, or intersystem communication for working with a cluster.

***SnowFall***: The Snowfall API uses list functions for parallelization. Calculations are distributed onto workers where each worker gets a portion of the full data to work with. The snowfall API is very similar to the snow API and has the following features:

Functions for loading packages and sources in the cluster
Functions for exchanging variables between cluster nodes.
All wrapper functions contain extended error handling.
Changing cluster settings does not require changing R code.
Can be done from command line.
All functions work in sequential mode as well. Switching between modes requires no change in R code.

###sfCluster

Generally in a cluster, all nodes have comparable performance specifications and each node will take approximately the same time to run.sfCluster wants to make sure that your cluster can meet your needs without exhausting resources.

***The Advantages of using sfCluster***

It checks your cluster to find machines with free resources if available. The available machines (or even the available parts of the machine) are built into a new sub-cluster which belongs to the new program.
It can optionally monitor the cluster for usage and stop programs if they exceed their memory allotment, disk allotment, or if machines start to swap.
It also provides diagnostic information about the current running clusters and free resources on the cluster including PIDs, memory use and runtime.

2.***Implicit Parallelism*** : Unlike explicit parallelism where the user controls (and can mess up) most of the cluster settings, with implicit parallelism most of the messy legwork in setting up the system and distributing data is avoided.


```{r}
library(snowfall) 
# 1. Initialisation of snowfall. 
# (if used with sfCluster, just call sfInit()) 
sfInit(parallel=F, cpus=NULL,type = "SOCK")




# 2. Loading data. 
require(mvna) 
data("sir.adm")
# 3. Wrapper, which can be parallelised. 
wrapper <- function(idx) { 
  # Output progress in worker logfile 
  cat( "Current index: ", idx, "\n" ) 
  index <- sample(1:nrow(sir.adm), replace=TRUE) 
  temp <- sir.adm[index, ] 
  fit <- crr(temp$time, temp$status, temp$pneu) 
  return(fit$coef) 
} 
# 4. Exporting needed data and loading required 
# packages on workers. 
sfExport("sir.adm") 
sfLibrary(cmprsk) 

# 5. Start network random number generator 
# (as "sample" is using random numbers). 
sfClusterSetupRNG() 
# 6. Distribute calculation


start <- Sys.time(); 
```
```{r results='hide'}
result <- sfLapply(1:1000, wrapper) ;
```

```{r}
Sys.time()-start
# Result is always in list form. 
mean(unlist(result)) 
# 7. Stop snowfall 
sfStop() 

```

```{r}
# example process:
process <- function(parallel = FALSE,cpus = NULL){
  sfInit(parallel = parallel, cpus = cpus)
  sfLapply( 1:10^6, log10 )
  sfStop()
}

# computational time in sequential mode:

system.time(process(parallel = FALSE))

system.time(process(parallel =TRUE,cpus=2))

```


#HadoopStreaming


MapReduce is a way of dividing a large job into many smaller jobs producing an output, and then combining the individual outputs into one output. It is a classic divide and conquer approach that is parallel and can easily be distributed among many cores or a CPU, or among many CPUs and nodes.

mapReduce is a pure R implementation of MapReduce. Many authors state that mapReduce is simply:

apply(map(data), reduce)

By default, mapReduce uses the same parallelization functionality as sapply.

For Example: mapReduce (map, ., data, apply = sapply)

In Simple terms:

***Map***: Performs operations such as filtering and sorting of the data
***Reduce***: Performs Summary operations on the mapped data


***Hadoop***:  Hadoop is an open-source implementation of MapReduce that has gained a lot of traction in the data mining community. Hadoop is distributed with Hadoop Streaming, which allows map/reduce jobs to be written in any language including R.

**The hadoopstreaming package is used in R to run map/reduce.**



#Rhipe

***Rhipe*** is another R interface for Hadoop that provides following advantages :
Incorporates an rhlapply function similar to the standard apply variants.
Uses Google Protocol Buffers.
Great flexibility in modifying Hadoop parameters.
At the end of the day, which package in R will you choose to handle Big Data?

For datasets with size in the range 10GB, bigmemory and ff handle themselves well.
For Datasets in the range of TB and PB, you can interface Hadoop in R



#RHadoop

***RHadoop*** is a collection of five R packages that allow users to manage and analyze data with Hadoop. The packages have been tested (and always before a release) on recent releases of the Cloudera and Hortonworks Hadoop distributions and should have broad compatibility with open source Hadoop and mapR's distribution. We normally test on recent Revolution R/Microsoft R and CentOS releases, but we expect all the RHadoop packages to work on a recent release of open source R and Linux. 

***rhdfs*** This package provides basic connectivity to the Hadoop Distributed File System. R programmers can browse, read, write, and modify files stored in HDFS from within R. Install this package only on the node that will run the R client.

***rhbase*** This package provides basic connectivity to the HBASE distributed database, using the Thrift server. R programmers can browse, read, write, and modify tables stored in HBASE from within R. Install this package only on the node that will run the R client. 

***plyrmr*** This package enables the R user to perform common data manipulation operations, as found in popular packages such as plyr and reshape2, on very large data sets stored on Hadoop. Like rmr, it relies on Hadoop MapReduce to perform its tasks, but it provides a familiar plyr-like interface while hiding many of the MapReduce details. Install this package only every node in the cluster. rmr2 A package that allows R developer to perform statistical analysis in R via Hadoop MapReduce functionality on a Hadoop cluster. Install this package on every node in the cluster.

***ravro*** A package that adds the ability to read and write avro files from local and HDFS file system and adds an avro input format for rmr2. Install this package only on the node that will run the R client.

